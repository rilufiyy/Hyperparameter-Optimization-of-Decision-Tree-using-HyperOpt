# -*- coding: utf-8 -*-
"""HyperOpt DT (TPE, Random) + Grid PIMA dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8dkALC5PM4bA1d94Ipj4V4NcUFJikPI
"""

import warnings
warnings.filterwarnings("ignore")

"""# Load Libraries"""

# Hyperopt core
from hyperopt import fmin, tpe, rand, atpe, hp, Trials, STATUS_OK
from hyperopt.base import Domain
from hyperopt.pyll.stochastic import sample

# Grid search (but limited support in Hyperopt)
from hyperopt import space_eval

# Sklearn (dataset, model, CV, metric)
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import recall_score, make_scorer
from sklearn.model_selection import train_test_split

# Utils
import numpy as np
import matplotlib.pyplot as plt

"""# Load Dataset Pima Diabetes"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
from sklearn.exceptions import ConvergenceWarning

# URL dataset di GitHub (gunakan raw link)
url = "https://raw.githubusercontent.com/rilufiyy/GWO-with-Levy-Flight---Pima-Diabetes/main/diabetes.csv"

# Tampilkan
df = pd.read_csv(url)
df.head()

import numpy as np

# Ganti nilai 0 pada kolom Glucose, BloodPressure, SkinThickness, Insulin, BMI menjadi NaN
cols_with_invalid_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_with_invalid_zeros] = df[cols_with_invalid_zeros].replace(0, np.nan)

# Cek Apakah ada missing value pada dataset
missing_value = df.isnull().sum()
print(missing_value)

# Buat variabel dataframe yang dibersihkan
df_cleaned = df.copy()

# Imputasi nilai tidak valid (nan) sesuai distribusi data
df_cleaned['Glucose'].fillna(df_cleaned['Glucose'].mean(), inplace = True)
df_cleaned['BloodPressure'].fillna(df_cleaned['BloodPressure'].mean(), inplace = True)
df_cleaned['SkinThickness'].fillna(df_cleaned['SkinThickness'].median(), inplace = True)
df_cleaned['Insulin'].fillna(df_cleaned['Insulin'].median(), inplace = True)
df_cleaned['BMI'].fillna(df_cleaned['BMI'].median(), inplace = True)

df_cleaned.info()

"""# Splitting Data"""

X = df_cleaned.drop('Outcome', axis=1)
y = df_cleaned['Outcome']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42
)

# Hitung jumlah data
total_data = df_cleaned.shape[0]
train_data = X_train.shape[0]
test_data = X_test.shape[0]

# Hitung persentase
train_pct = (train_data / total_data) * 100
test_pct = (test_data / total_data) * 100

# Tampilkan hasil
print(f'Jumlah data awal        : {total_data} baris')
print(f'Jumlah data latih       : {train_data} baris ({train_pct:.2f}%)')
print(f'Jumlah data uji         : {test_data} baris ({test_pct:.2f}%)')

"""# Data Scaling"""

# Scaling data menggunakan MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Data Resampling"""

# Cek jumlah outcome pada data latih
y_train.value_counts()

from imblearn.combine import SMOTETomek
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Tampilkan distribusi kelas sebelum resampling
print("Distribusi kelas sebelum resampling:")
print(pd.Series(y_train).value_counts())

# Lakukan resampling
smote_tomek = SMOTETomek(random_state=42)
X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train_scaled, y_train)

# Tampilkan distribusi kelas setelah resampling
print("\nDistribusi kelas setelah resampling:")
print(pd.Series(y_train_resampled).value_counts())

# Visualisasi distribusi sebelum dan sesudah
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.set_style("whitegrid")

# Sebelum resampling
sns.countplot(x=y_train, ax=axes[0], palette="pastel")
axes[0].set_title("Sebelum Resampling")
axes[0].set_xlabel("Kelas")
axes[0].set_ylabel("Jumlah")

# Setelah resampling
sns.countplot(x=y_train_resampled, ax=axes[1], palette="pastel")
axes[1].set_title("Setelah Resampling (SMOTE + Tomek)")
axes[1].set_xlabel("Kelas")
axes[1].set_ylabel("Jumlah")

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from hyperopt import fmin, tpe, rand, hp, Trials, STATUS_OK
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import recall_score, make_scorer

# Definisikan search space untuk hyperparameter Decision Tree
space = {
    'max_depth': hp.choice('max_depth', range(2, 51)),
    'min_samples_split': hp.choice('min_samples_split', range(2, 31)),
    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 21)),
    'criterion': hp.choice('criterion', ["gini", "entropy", "log_loss"])
}

# Definisikan fungsi objektif
def objective(params):
    clf = DecisionTreeClassifier(
        max_depth=params['max_depth'],
        min_samples_split=params['min_samples_split'],
        min_samples_leaf=params['min_samples_leaf'],
        criterion=params['criterion'],
        class_weight="balanced",   # penting untuk dataset tidak seimbang
        random_state=42
    )

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    recall = cross_val_score(
        clf, X, y, cv=cv, scoring=make_scorer(recall_score, pos_label=1)
    ).mean()

    return {'loss': -recall, 'status': STATUS_OK}

# TPE HyperOpt
trials_tpe = Trials()   # konsisten nama variabel
best_tpe = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,   # pakai TPE
    max_evals=300,
    trials=trials_tpe,
    rstate=np.random.default_rng(42)
)

# Hitung best recall
best_recall_tpe = -min(t['result']['loss'] for t in trials_tpe.trials)
print("Best params:", best_tpe)
print("Best Recall (CV):", best_recall_tpe)

# Random Search on HyperOpt
trials_rand = Trials()
best_rand = fmin(fn=objective, space=space, algo=rand.suggest, max_evals=300, trials=trials_rand)
best_recall_rand = -min(t["result"]["loss"] for t in trials_rand.trials)
print("Random best:", best_rand, "| Recall:", best_recall_rand)

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import recall_score, make_scorer
from sklearn.tree import DecisionTreeClassifier

param_grid = {
    'max_depth': [2, 51],
    'min_samples_split': [2, 31],
    'min_samples_leaf': [1, 21],
    'criterion': ["gini", "entropy", "log_loss"]
}

clf = DecisionTreeClassifier(class_weight="balanced", random_state=42)
grid_search = GridSearchCV(
    clf, param_grid, cv=5, scoring=make_scorer(recall_score), n_jobs=-1
)
grid_search.fit(X, y)

# Simulasi trials_grid supaya bisa divisualisasi serupa
trials_grid = []
for mean_score, params in zip(grid_search.cv_results_['mean_test_score'],
                              grid_search.cv_results_['params']):
    trials_grid.append({'result': {'loss': -mean_score}, 'params': params})

# Best dari grid search
best_grid = grid_search.best_params_
best_recall_grid = grid_search.best_score_

print("Grid best:", best_grid, "| Recall:", best_recall_grid)

"""# Visualisasi History Training"""

import matplotlib.pyplot as plt
import numpy as np

def plot_optimization_history(trials, title, from_grid=False):
    if from_grid:
        recalls = [-t['result']['loss'] for t in trials]
    else:
        recalls = [-t['result']['loss'] for t in trials.trials]

    best_so_far = np.maximum.accumulate(recalls)

    plt.figure(figsize=(10,6))
    plt.scatter(range(1, len(recalls)+1), recalls, label="Objective Value", color="blue", alpha=0.6)
    plt.plot(range(1, len(best_so_far)+1), best_so_far, label="Best Value", color="red", linewidth=2)
    plt.xlabel("Trial")
    plt.ylabel("Recall (CV mean)")
    plt.title(f"Optimization History - {title}")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.show()

# HyperOpt TPE
plot_optimization_history(trials_tpe, "TPE")

# HyperOpt Random Search
plot_optimization_history(trials_rand, "Random Search")

# Grid search visualization
plot_optimization_history(trials_grid, "Grid Search", from_grid=True)

best_scores = {
    "TPE": best_recall_tpe,
    "Random": best_recall_rand,
    "Grid": best_recall_grid
}

plt.figure(figsize=(6,5))
sns.barplot(x=list(best_scores.keys()), y=list(best_scores.values()))
plt.title("Best Recall from Each Search Method")
plt.ylabel("Recall (CV mean)")
plt.show()

"""## Report Result"""

from hyperopt import space_eval
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Decode best params dari masing-masing optimizer
best_params_tpe   = space_eval(space, best_tpe)
best_params_rand  = space_eval(space, best_rand)
# GridSearchCV sudah langsung simpan best_params_
best_params_grid  = grid_search.best_params_

# Dictionary untuk mempermudah looping
best_models = {
    "TPE (Hyperopt)": best_params_tpe,
    "Random (Hyperopt)": best_params_rand,
    "GridSearchCV": best_params_grid
}

for name, params in best_models.items():
    # Buat model
    model = DecisionTreeClassifier(
        **params,
        class_weight="balanced",
        random_state=42
    )
    model.fit(X_train, y_train)

    # Prediksi
    y_pred = model.predict(X_test)

    # Classification report
    print(f"\n=== Classification Report ({name}) ===")
    print(classification_report(y_test, y_pred, target_names=['No Diabetes', 'Diabetes']))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Diabetes', 'Diabetes'],
                yticklabels=['No Diabetes', 'Diabetes'],
                cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()